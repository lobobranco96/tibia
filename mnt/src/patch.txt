Historico de mudanças:

21/10

- extract.py: Adicionado ao Vocation().processar_paginas() e Category.processar_paginas(), retries na tentativa de coletar os dados e falhar por timeout.
- utility.py: Fix no CSVLanding, ajuste na forma em que o CSV é salvo no diretório minio/landing.
- Refatoração da lógica entre Landing e Bronze. Agora os scripts que fazem a coleta dos dados fazem parte da camada Landing.
- tests/: Mudança na importação dos módulos, ajuste para .landing.
- landing_app: Documentação atualizada.
- highscore_pipeline.py: Fix na importação, agora from src.landing.landing_app.
- Adicionado script PySpark para Bronze:
  - Vocation com Iceberg e Nessie.
- Função modular para criação da Spark Session.

23/10

- Camada Silver implementada com script PySpark utilizando Iceberg e Nessie, seguindo a arquitetura medalhão.
- Implementação do SCD Type 2 na Silver para controlar histórico de mudanças dos jogadores.
- Alterações para detectar e registrar mudanças nas colunas vocation e world além de level e experience.
- Partitionamento otimizado na tabela Silver usando world, days(start_date) e bucket(8, name).
- Uso do comando MERGE INTO para upsert eficiente e manutenção do histórico.
- Inclusão de logging detalhado para monitoramento das etapas do pipeline Silver.
- Passagem opcional de parâmetro --date para execução parametrizada por data.
- DAG lakehouse_pipeline criada, acionada somente após finalização da DAG landing_highscores_pipeline.
- Configuração de SparkSubmitOperator atualizada para incluir todos os jars necessários:
  - hadoop-aws, aws-java-sdk-bundle, iceberg-spark-runtime, nessie-spark-extensions, iceberg-aws-bundle, aws-sdk-bundle.
- Jobs Bronze (Vocation, Skills, Extra) parametrizados e independentes, permitindo paralelismo.
- Integração completa com Iceberg + Nessie para versionamento e governança das tabelas Silver e Gold.
- Explicação da DAG de extração e DAG Lakehouse adicionada ao README.

27/10

- Adicionada coluna batch_id nas tabelas Bronze (vocation e skills) para rastreabilidade de execução.
- Implementado job PySpark Bronze Skills com Iceberg e Nessie:
  - Padronização de schema (name, vocation, world, level, skill_level, category, ingestion_time, ingestion_date, source_system, batch_id).
  - Particionamento em (world, ingestion_date) e compressão Snappy.
- Criada camada Silver para Skills com histórico completo (SCD Type 2) via MERGE INTO, mantendo a mesma estrutura da Silver de Vocation:
  - Controle de mudanças em skill_level, level, vocation e world.
  - Colunas start_date, end_date, is_current adicionadas.
- Atualização do README da pasta src/:
  - Inclusão da documentação completa das camadas Bronze e Silver para skills.
  - Descrição detalhada da rastreabilidade e versionamento com SCD Type 2.
  - Inclusão das novas colunas (batch_id, is_current, start_date, end_date).
- Logging aprimorado em todas as camadas para melhor auditoria do fluxo de dados.
- Pipeline consolidado para ingestão e versionamento de vocation e skills em Iceberg + Nessie, garantindo governança total dos dados.