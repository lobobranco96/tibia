{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fb9778-bf02-4d9f-bad1-95c4eeafa169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "S3_ENDPOINT = os.getenv(\"S3_ENDPOINT\")\n",
    "NESSIE_URI = os.getenv(\"NESSIE_URI\")\n",
    "\n",
    "\n",
    "def create_spark_session(appname):\n",
    "    master = \"spark://spark-master:7077\"\n",
    "\n",
    "    conf = (\n",
    "        pyspark.SparkConf()\n",
    "        .setAppName(appname)\n",
    "        .set(\"spark.master\", master)\n",
    "\n",
    "        # EXTENSÕES ICEBERG + NESSIE\n",
    "        .set(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        )\n",
    "\n",
    "        # REGISTRO DO CATÁLOGO NESSIE\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.type\", \"nessie\")\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\"spark.sql.catalog.nessie.cache-enabled\", \"false\")\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", \"s3a://lakehouse/\")\n",
    "\n",
    "        # CONFIG S3 -> ICEBERG\n",
    "        .set(\"spark.sql.catalog.nessie.s3.path-style-access\", \"true\")\n",
    "        .set(\"spark.sql.catalog.nessie.s3.endpoint\", S3_ENDPOINT)\n",
    "\n",
    "        # CONFIG HADOOP S3A\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\n",
    "                \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "                \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    "            )\n",
    "        .set(\"spark.hadoop.fs.defaultFS\", \"s3a://lakehouse\")\n",
    "\n",
    "        # RECURSOS\n",
    "        .set(\"spark.executor.memory\", \"1g\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.driver.memory\", \"1g\")\n",
    "        .set(\"spark.executor.instances\", \"1\")\n",
    "    )\n",
    "\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    return spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3323cd1-b0fe-449a-b77b-0e0d3a18534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91af99f0-c3fe-4cb5-8cdb-33d4807f7213",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session(\"bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8252fe5c-9f83-4e89-bca8-2f69c1975055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5d6a0951e70b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bronze</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f610cf05490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd04ab13-af2d-4287-986a-28641bcc7471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "class Bronze:\n",
    "    \"\"\"\n",
    "    Classe responsável pela camada Bronze do Lakehouse Tibia.\n",
    "\n",
    "    A classe realiza:\n",
    "    - Configuração do catálogo Nessie + Iceberg.\n",
    "    - Criação dos namespaces e tabelas caso não existam.\n",
    "    - Leitura dos arquivos CSV da camada Landing.\n",
    "    - Padronização dos dados.\n",
    "    - Registro de metadados operacionais.\n",
    "    - Escrita incremental via Iceberg (append).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Sessão Spark ativa criada no job.\n",
    "    date_str : str | None\n",
    "        Data fornecida via CLI no formato 'YYYY-MM-DD'.\n",
    "        Caso None, utiliza a data atual para construir o path da camada Landing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, spark, date_str):\n",
    "        self.spark = spark\n",
    "        self.date_str = date_str\n",
    "\n",
    "    # ============================================================\n",
    "    #   MÉTODO: VOCATION\n",
    "    # ============================================================\n",
    "    def vocation(self):\n",
    "        \"\"\"\n",
    "        Executa o job Bronze para dados de vocação (vocation).\n",
    "\n",
    "        1. Configura catálogo Iceberg + warehouse.\n",
    "        2. Cria namespace + tabela caso não existam.\n",
    "        3. Lê arquivos CSV da camada landing.\n",
    "        4. Valida colunas obrigatórias.\n",
    "        5. Normaliza nomes e tipos de colunas.\n",
    "        6. Gera batch_id e colunas de auditoria.\n",
    "        7. Deduplica por (name, world).\n",
    "        8. Insere incrementalmente na tabela Bronze.\n",
    "\n",
    "        Tabela criada: nessie.bronze.vocation\n",
    "        \"\"\"\n",
    "\n",
    "        # Namespace\n",
    "        self.spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.bronze\")\n",
    "\n",
    "        # Criação da tabela Iceberg\n",
    "        self.spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS nessie.bronze.vocation (\n",
    "            name STRING,\n",
    "            vocation STRING,\n",
    "            level INT,\n",
    "            world STRING,\n",
    "            experience LONG,\n",
    "            world_type STRING,\n",
    "            ingestion_time TIMESTAMP,\n",
    "            ingestion_date DATE,\n",
    "            source_system STRING,\n",
    "            batch_id STRING\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY (world, ingestion_date)\n",
    "        TBLPROPERTIES (\n",
    "            'format-version' = '2',\n",
    "            'write.format.default' = 'parquet',\n",
    "            'write.metadata.compression' = 'gzip',\n",
    "            'write.delete.mode' = 'merge-on-read'\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Define path da landing com base na data\n",
    "        today_date = datetime.strptime(self.date_str, \"%Y-%m-%d\") if self.date_str else datetime.today()\n",
    "        partition = f\"year={today_date.year}/month={today_date.month}/day={today_date.day}\"\n",
    "\n",
    "        path = f\"s3a://lakehouse/landing/{partition}/experience/\"\n",
    "        logging.info(f\"Lendo dados de: {path}\")\n",
    "\n",
    "        # Lê arquivo CSV\n",
    "        df_raw = self.spark.read.csv(path, header=True)\n",
    "\n",
    "        # Validação de colunas esperadas\n",
    "        colunas_esperadas = {\"Rank\", \"Name\", \"Vocation\", \"World\", \"Level\", \"Points\", \"WorldType\"}\n",
    "        colunas_faltando = colunas_esperadas - set(df_raw.columns)\n",
    "\n",
    "        if colunas_faltando:\n",
    "            logging.error(f\"Colunas ausentes no CSV: {colunas_faltando}\")\n",
    "            return\n",
    "\n",
    "        # Gera batch_id para auditoria\n",
    "        batch_id = str(uuid4())\n",
    "        logging.info(f\"Gerando batch_id: {batch_id}\")\n",
    "\n",
    "        df_raw.printSchema()\n",
    "\n",
    "        # Normalização e padronização\n",
    "        df_bronze = (\n",
    "            df_raw.drop(\"Rank\")\n",
    "            .withColumnRenamed(\"Name\", \"name\")\n",
    "            .withColumnRenamed(\"Vocation\", \"vocation\")\n",
    "            .withColumnRenamed(\"Level\", \"level\")\n",
    "            .withColumnRenamed(\"World\", \"world\")\n",
    "            .withColumnRenamed(\"Points\", \"experience\")\n",
    "            .withColumnRenamed(\"WorldType\", \"world_type\")\n",
    "            .withColumn(\"ingestion_time\", F.current_timestamp())\n",
    "            .withColumn(\"ingestion_date\", F.current_date())\n",
    "            .withColumn(\"source_system\", F.lit(\"highscore_tibia_page\"))\n",
    "            .withColumn(\"batch_id\", F.lit(batch_id))\n",
    "            .withColumn(\"experience\", F.regexp_replace(\"experience\", \",\", \"\").cast(\"long\"))\n",
    "            .withColumn(\"level\", F.col(\"level\").cast(\"int\"))\n",
    "            .withColumn(\"vocation\", F.trim(F.lower(\"vocation\")))\n",
    "            .withColumn(\"world\", F.trim(F.lower(\"world\")))\n",
    "            .dropDuplicates([\"name\", \"world\"])\n",
    "        )\n",
    "\n",
    "        # Compressão padrão Parquet\n",
    "        self.spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    "        record_count = df_bronze.count()\n",
    "\n",
    "        if record_count > 0:\n",
    "            logging.info(f\"Inserindo {record_count} registros na Bronze com batch_id {batch_id}...\")\n",
    "            df_bronze.writeTo(\"nessie.bronze.vocation\").append()\n",
    "        else:\n",
    "            logging.warning(\"Nenhum registro encontrado para gravar na Bronze.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "021b9c8c-7b88-4651-8eef-ea1d88329bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bronze = Bronze(spark, \"2025-12-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e290d138-cc42-47a5-b117-3a4c5dc7fbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 23:57:23,117 - INFO - Lendo dados de: s3a://lakehouse/landing/year=2025/month=12/day=25/experience/\n",
      "2025-12-25 23:57:23,797 - INFO - Gerando batch_id: e852ac20-c5d1-4d23-9c31-ae84045a88af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rank: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Vocation: string (nullable = true)\n",
      " |-- World: string (nullable = true)\n",
      " |-- Level: string (nullable = true)\n",
      " |-- Points: string (nullable = true)\n",
      " |-- WorldType: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-25 23:57:25,394 - INFO - Inserindo 500 registros na Bronze com batch_id e852ac20-c5d1-4d23-9c31-ae84045a88af...\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bronze.vocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c0aa5ea-5ab4-4c35-bdcf-4598d5930314",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52c8f2ff-e53b-4f84-bbec-1db76ece70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+-------+-----------+--------------+--------------------+--------------+--------------------+--------------------+\n",
      "|         name|   vocation|level|  world| experience|    world_type|      ingestion_time|ingestion_date|       source_system|            batch_id|\n",
      "+-------------+-----------+-----+-------+-----------+--------------+--------------------+--------------+--------------------+--------------------+\n",
      "|   Appov Boss|elder druid| 1017|eclipta|17452496195|Retro Open PvP|2025-12-25 23:47:...|    2025-12-25|highscore_tibia_page|f782c035-7bfc-4a0...|\n",
      "|        Aztev|elder druid| 1025|eclipta|17893581139|Retro Open PvP|2025-12-25 23:47:...|    2025-12-25|highscore_tibia_page|f782c035-7bfc-4a0...|\n",
      "|   Bre no zin|elder druid|  874|eclipta|11084969190|Retro Open PvP|2025-12-25 23:47:...|    2025-12-25|highscore_tibia_page|f782c035-7bfc-4a0...|\n",
      "|Brunon Brunon|elder druid| 1168|eclipta|26432758562|Retro Open PvP|2025-12-25 23:47:...|    2025-12-25|highscore_tibia_page|f782c035-7bfc-4a0...|\n",
      "| Dino Machine|elder druid|  976|eclipta|15425632446|Retro Open PvP|2025-12-25 23:47:...|    2025-12-25|highscore_tibia_page|f782c035-7bfc-4a0...|\n",
      "+-------------+-----------+-----+-------+-----------+--------------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"nessie.bronze.vocation\")\n",
    "df.count().show(5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf56f53a-d408-48b7-94fe-b868282afa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ab0365a-3b61-4483-9a92-1c7fbc09c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2780755d-62b5-4217-b496-b6f934187501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
