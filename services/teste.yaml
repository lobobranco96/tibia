x-common-config: &default_config
  networks:
    - lakehouse_network
  env_file:
    - .credentials.conf
  healthcheck:
    test: ["CMD", "ls"]
    timeout: 45s
    interval: 10s
    retries: 10
    start_period: 10s

x-spark-worker-default: &spark_worker_default
  build:
    context: ../docker/spark
    dockerfile: Dockerfile
  <<: *default_config
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER=spark://spark-master:7077
    - SPARK_WORKER_MEMORY=2G
    - SPARK_WORKER_CORES=2
  command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
  networks:
    - lakehouse_network

services:
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    <<: *default_config
    command: server /data --console-address ":9001"
    volumes:
      - lakehouse_minio_vol:/data
      - ../mnt/minio:/data
      - ../mnt/minio/minio-client:/root/.mc
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-bucket:
    image: minio/mc:latest
    container_name: minio-bucket
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      sleep 10;
      until mc alias set myminio http://minio:9000 admin password >/dev/null 2>&1; do
        echo 'Aguardando MinIO...';
        sleep 5;
      done;
      echo 'MinIO está pronto. Iniciando configuração dos buckets.';
      mc mb myminio/lakehouse;
      echo 'Bucke criados com sucesso.';
      mc anonymous set public myminio/lakehouse;
      echo 'Políticas de acesso definidas para público.';
      
      mc cp /dev/null myminio/lakehouse/landing/placeholder
      mc cp /dev/null myminio/lakehouse/bronze/placeholder
      mc cp /dev/null myminio/lakehouse/silver/placeholder
      mc cp /dev/null myminio/lakehouse/gold/placeholder
      echo 'camadas criadas com sucesso.';
      "
    <<: *default_config

  postgres_nessie:
    image: postgres:16
    container_name: postgres_nessie
    <<: *default_config
    environment:
      POSTGRES_USER: nessie
      POSTGRES_PASSWORD: nessie
      POSTGRES_DB: nessie
    volumes:
      - lakehouse_pg_nessie_vol:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U nessie"]
      interval: 5s
      timeout: 5s
      retries: 10

  nessie:
    image: ghcr.io/projectnessie/nessie:0.90.4
    container_name: nessie
    restart: always
    depends_on:
      postgres_nessie:
        condition: service_healthy
    environment:
      QUARKUS_HTTP_PORT: "19120"

      NESSIE_VERSION_STORE_TYPE: JDBC
      NESSIE_VERSION_STORE_JDBC_DATABASE: POSTGRES
      NESSIE_VERSION_STORE_JDBC_INIT: "true"

      QUARKUS_DATASOURCE_DB_KIND: postgresql
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres_nessie:5432/nessie
      QUARKUS_DATASOURCE_USERNAME: nessie
      QUARKUS_DATASOURCE_PASSWORD: nessie

    ports:
      - "19120:19120"
    networks:
      - lakehouse_network

  notebook:
    image: spark-custom:3.5.3
    build: ../docker/notebook
    container_name: notebook
    <<: *default_config
    ports:
      - 8888:8888
    volumes:
      - ../mnt/notebooks:/app/
    environment:
      - NESSIE_URI=http://nessie:19120/api/v1
      - SPARK_SUBMIT_OPTIONS=--conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
                            --conf spark.hadoop.fs.s3a.path.style.access=true \
                            --conf spark.hadoop.fs.s3a.access.key=admin \
                            --conf spark.hadoop.fs.s3a.secret.key=password

  spark-master:
    build: ../docker/spark
    container_name: spark-master
    <<: *default_config
    environment:
      - SPARK_MODE=master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark-master"]
    volumes:
      - ../mnt/src/jobs:/spark
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - lakehouse_network

  spark-worker-1:
    container_name: spark-worker-1
    <<: *spark_worker_default
    depends_on:
      - spark-master
      
  spark-worker-2:
    container_name: spark-worker-2
    <<: *spark_worker_default
    depends_on:
      - spark-master

volumes:
  lakehouse_minio_vol:
  lakehouse_pg_nessie_vol:


networks:
  lakehouse_network:

    driver: bridge

